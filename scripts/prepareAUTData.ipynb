{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "data_dir = Path('~/drive/Projects/backups/ocs/ocsai-py/data/ocsai1')\n",
    "\n",
    "splits = dict()\n",
    "for (split, fname) in [('train', 'finetune-gt_main2_prepared_train.jsonl'), ('val', 'finetune-gt_main2_prepared_val.jsonl')]:\n",
    "    df = pd.read_json(data_dir / fname, lines=True)\n",
    "\n",
    "    # Split the 'prompt' column on newlines and extract the relevant parts\n",
    "    df[['prompt', 'response']] = df['prompt'].str.extract(r'AUT Prompt:(.+)\\nResponse:(.+)\\nScore:', expand=True)\n",
    "\n",
    "    df['prompt'] = df['prompt'].str.strip()\n",
    "    df['response'] = df['response'].str.strip()\n",
    "    df.completion = df.completion.astype(int).div(10)\n",
    "    df = df.rename(columns={'completion': 'score'})\n",
    "    splits[split] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "saved_prompts = []\n",
    "\n",
    "# Group by prompt and create separate JSON files\n",
    "for prompt in splits['val']['prompt'].unique():\n",
    "    # Filter data for this prompt\n",
    "    prompt_data = splits['val'][splits['val']['prompt'] == prompt]\n",
    "\n",
    "    # Limit to 100 examples, randomized\n",
    "    n_examples = 100\n",
    "    if prompt_data.shape[0] > n_examples:\n",
    "        prompt_data = prompt_data.sample(n_examples, random_state=1234)\n",
    "        saved_prompts.append(prompt)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    # Create the examples list\n",
    "    examples = [\n",
    "        {\n",
    "            \"text\": row['response'],\n",
    "            \"truth\": row['score']  # assuming 'score' is your truth column\n",
    "        }\n",
    "        for _, row in prompt_data.iterrows()\n",
    "    ]\n",
    "    \n",
    "    # Create the JSON structure\n",
    "    dataset = {\n",
    "        \"description\": f\"This dataset contains responses to the question, 'What is a surprising use for a {prompt}?'. It is sourced from multiple studies, with different participant groups and different experimental conditions. The responses are scores by human judges on a scale of 1-5, with 1 being the least original and 5 being the most original. The ground truth is an average of multiple judges, rounded to one decimal point.\",\n",
    "        \"train_data_url\": f\"https://github.com/organisciak/ai-class/tree/main/static/datasets/aut_{prompt}_analysis.md\",\n",
    "        \"examples\": examples\n",
    "    }\n",
    "    \n",
    "    # Create filename - convert prompt to lowercase and remove spaces\n",
    "    filename = f\"aut_{prompt.lower().replace(' ', '_')}.json\"\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs('../src/lib/data/datasets', exist_ok=True)\n",
    "    \n",
    "    # Write the JSON file\n",
    "    with open(f'../src/lib/data/datasets/{filename}', 'w') as f:\n",
    "        json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save train dataset to an easy to reference file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create markdown and CSV files for train data analysis\n",
    "for prompt in saved_prompts:\n",
    "    # Filter train data for this prompt\n",
    "    prompt_data = splits['train'][splits['train']['prompt'] == prompt]\n",
    "    \n",
    "    # Sort by score for percentile analysis\n",
    "    sorted_data = prompt_data.sort_values('score', ascending=False)\n",
    "    \n",
    "    # Calculate indices for different sections\n",
    "    n_five_percent = int(n_examples * 0.05)\n",
    "    top_responses = sorted_data.head(n_five_percent)\n",
    "    bottom_responses = sorted_data.tail(n_five_percent)\n",
    "    \n",
    "    # Get median responses (middle 5 responses)\n",
    "    median_idx = n_examples // 2\n",
    "    median_responses = sorted_data.iloc[median_idx-2:median_idx+3]\n",
    "    \n",
    "    # Create markdown content\n",
    "    markdown_content = f\"\"\"# AUT Analysis: {prompt}\n",
    "\n",
    "## Examples: Top Responses\n",
    "{top_responses[['response', 'score']].to_markdown(index=False)}\n",
    "\n",
    "## Examples: Bottom Responses\n",
    "{bottom_responses[['response', 'score']].to_markdown(index=False)}\n",
    "\n",
    "## Median Responses\n",
    "{median_responses[['response', 'score']].to_markdown(index=False)}\n",
    "\n",
    "## All Train Examples\n",
    "\n",
    "```csv\n",
    "{prompt_data[['response', 'score']].to_csv(index=False).strip()}\n",
    "```\n",
    "\"\"\"\n",
    "    \n",
    "    # Save markdown file\n",
    "    markdown_filename = f\"aut_{prompt.lower().replace(' ', '_')}_analysis.md\"\n",
    "    with open(f'../static/datasets/{markdown_filename}', 'w') as f:\n",
    "        f.write(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
